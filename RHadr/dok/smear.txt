First let me stress that machine beam energy spread was meant for LEP
physics. At the time of its implementation I never considered resonances
like psi or phi. I had in mind Z resonance, for which the beam spread is
much smaller than the Z resonance width.
I have never tested RRes package with nonzero beam spread DelEne =xpar( 2).

Now let me elaborate on several techical points, and the code.
The gausian beam energy random variation is realized using
      SUBROUTINE KarLud_SmearBeams
in file KK2f/Karlud.f

I can see immediately that the comment in front of this routine
*//    This is correct only for very small spread < 2GeV                            //
is not correct because:
- The beam spread should be small compared to resonance width. 2GeV is here Z width.
  For low energies it should be the width of the resonances 
- If the above is not true then the actual MC implemention is not "INCORRECT"
  (although it can be, see below) but rather it is mathematically correct
  but its implementation is "INEFFICIENT", from the point of view of weight variation.

Let me elaborate a litle bit more on the above:
In the MC algorithm for ISR (see KK2f/Karlud.f, bornv/BornV.f) we memorize
photon spectrum f(v)=rho_isr(v)*Born(s(1-v)). In the presence of the narrow resonances
this function has peaks at v_i=1-M_i^2/s, where M_i is mass of the resonance.
This f(v) is memorized once for ever in the MC initialization phase.
In the presence of the machine beam spread (MBS) we cannot change E=sqrt(s)
event-per-event, because changing s redefines f(v), in particular positions v_i
of the peaks will moove! What I said is only half true -- we can change s, but
we have to comensate this change EXACTLY by the MC weight which involves
the ration w=f(v_X)/f(v), where v_X = 1-s(1-v)/s_X is shifted.
This weight w will contain in particulare the ratio of two Breit-Wigners.
Mathematicaly this solution is correct, but obviously
if the beam spread is bigger than width of the resonance than the weight will go crazy.
One may live with the crazy MC weight and still get correct results for weighted
MC events. What is realy dangerous is to use WT=1 events.
In this case events m_WtMain > m_WTmax may get strongly distorded.
(The resonance may get severely distorted).
One method is to scrutinize and control very well statistics of the "overweighted
events" and if there is to many of them, to increase WTmax
see input data:
   9          1.0e0      WtMax =xpar( 9)   Maximum weight for rejection
and also
  517          5.0e0     WtMax Maximum weight for rejection d-quark
Unfortunately, setting WtMax=1000 or so can be very wasteful, in view of the
fact that the overveigted event concentrate in small part of the phase space
and we reject 999/1000 events everywhere (weight distribution will have long tail
and all of the tail will come from the narrow resonance -- a small fraction
of the total cross section!).

However, in KKMC I have implemented another interesting
protenction mechanism for such the case of "unweighted events":
see part of code in KK2f/KK2f.f
         WtScaled = m_WtMain/m_WTmax
         IF( WtScaled .GT. 1d0) THEN
            m_WtMain = WtScaled
         ELSE
            m_WtMain = 1.d0
         ENDIF
As you see abnormal events with m_WtMain > m_WTmax are still assigned the "scalled weight" 
m_WtMain>1. Most of events will have m_WtMain=1, but there might be a subset of "abnormal"
events with weight>1. It is perhaps not very convenient, but by keeping the record
of this weight allows safely to avoid the worst pitfall, leading to completely wrong result.
